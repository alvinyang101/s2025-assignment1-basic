{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from pathlib import Path\n",
    "import cProfile, pstats\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ece496b_basics.tokenizer import from_files, Tokenizer\n",
    "\n",
    "def get_compression_ratio(text: str, indices: list[int]) -> float:\n",
    "    \"\"\"Given `text` that has been tokenized into `indices`, .\"\"\"\n",
    "    num_bytes = len(bytes(text, encoding=\"utf-8\"))  # Original\n",
    "    num_tokens = len(indices)                       # Tokenized\n",
    "    return num_bytes / num_tokens\n",
    "\n",
    "DATA_PATH = Path(\"../data\").resolve()\n",
    "OUTPUT_PATH = Path(\"outputs\").resolve()\n",
    "tinystories_merges_path = OUTPUT_PATH / \"tinystories_merges.pkl\"\n",
    "tinystories_vocab_path = OUTPUT_PATH / \"tinystories_vocab.pkl\"\n",
    "owt_merges_path = OUTPUT_PATH / \"owt_merges.pkl\"\n",
    "owt_vocab_path = OUTPUT_PATH / \"owt_vocab.pkl\"\n",
    "\n",
    "ts_tokenizer = from_files(tinystories_vocab_path, tinystories_merges_path, special_tokens=[\"<|endoftext|>\"])\n",
    "owt_tokenizer = from_files(owt_vocab_path, owt_merges_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 10 MB snippets\n",
    "with open(DATA_PATH / \"TinyStoriesV2-GPT4-train.txt\", \"r\") as file:\n",
    "    lines = file.readlines(10485760)\n",
    "with open(DATA_PATH / \"tinystories_snippet.txt\", \"w\") as file:\n",
    "    file.writelines(lines)\n",
    "\n",
    "with open(DATA_PATH / \"owt_train.txt\", \"r\") as file:\n",
    "    lines = file.readlines(10485760)\n",
    "with open(DATA_PATH / \"owt_snippet.txt\", \"w\") as file:\n",
    "    file.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH / \"tinystories_snippet.txt\", \"r\") as file:\n",
    "    tinystories_snippet = file.read()\n",
    "with open(DATA_PATH / \"owt_snippet.txt\", \"r\") as file:\n",
    "    owt_snippet = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_encoded_withts = ts_tokenizer.encode(tinystories_snippet)\n",
    "ts_encoded_withowt = owt_tokenizer.encode(tinystories_snippet)\n",
    "\n",
    "owt_encoded_withts = ts_tokenizer.encode(owt_snippet)\n",
    "owt_encoded_withowt = owt_tokenizer.encode(owt_snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TinyStories encoded with TinyStories: 4.116450125906134\n",
      "TinyStories encoded with owt: 3.968451185686118\n",
      "owt encoded with TinyStories: 3.2057830352906818\n",
      "owt encoded with owt: 4.385997984719181\n"
     ]
    }
   ],
   "source": [
    "print(f\"TinyStories encoded with TinyStories: {get_compression_ratio(tinystories_snippet, ts_encoded_withts)}\")\n",
    "print(f\"TinyStories encoded with owt: {get_compression_ratio(tinystories_snippet, ts_encoded_withowt)}\")\n",
    "print(f\"owt encoded with TinyStories: {get_compression_ratio(owt_snippet, owt_encoded_withts)}\")\n",
    "print(f\"owt encoded with owt: {get_compression_ratio(owt_snippet, owt_encoded_withowt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH / \"TinyStoriesV2-GPT4-train.txt\", \"r\") as file:\n",
    "    token_ids = []\n",
    "    batch = \"\"\n",
    "    lines = file.readlines()\n",
    "    step_size = 100000\n",
    "    for idx in tqdm(range(0, len(lines), step_size)):\n",
    "        encoding = ts_tokenizer.encode(\"\".join(lines[idx:idx+step_size]))\n",
    "        token_ids.extend(encoding)\n",
    "    token_array = np.array(token_ids, dtype=np.uint16)\n",
    "    del lines\n",
    "\n",
    "with open(OUTPUT_PATH / \"tinystories_encoded.npy\", \"wb\") as file:\n",
    "    np.save(file, token_array)\n",
    "\n",
    "# with open(DATA_PATH / \"owt-train.txt\", \"r\") as file:\n",
    "#     token_ids = []\n",
    "#     batch = \"\"\n",
    "#     lines = file.readlines()\n",
    "#     step_size = 100000\n",
    "#     for idx in tqdm(range(0, len(lines), step_size)):\n",
    "#         encoding = ts_tokenizer.encode(\"\".join(lines[idx:idx+step_size]))\n",
    "#         token_ids.extend(encoding)\n",
    "#     token_array = np.array(token_ids, dtype=np.uint16)\n",
    "#     del lines\n",
    "#\n",
    "# with open(OUTPUT_PATH / \"owt_encoded.npy\", \"wb\") as file:\n",
    "#     np.save(file, token_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece496b_basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
